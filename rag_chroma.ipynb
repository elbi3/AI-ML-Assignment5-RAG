{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71238def-747f-4c17-9ebf-329ad7946658",
   "metadata": {},
   "source": [
    "# RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115d7177-2990-450b-8d97-e4f04fbca6aa",
   "metadata": {},
   "source": [
    "## Installs and checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e656b4a-4346-4b11-a67f-2ce716e0f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import chromadb\n",
    "import markdown2\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import shutil\n",
    "import stat\n",
    "import tiktoken\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d0f6f25-7b27-4c20-ab68-7a0793c0e384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  2.9.1+cu130\n",
      "CUDA available:  True\n",
      "GPU device name:  NVIDIA GeForce RTX 5070 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# check PyTorch and CUDA\n",
    "print(\"Torch version: \", torch.__version__)\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU device name: \", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75fa96bb-8beb-4d89-9752-9e15c11e3829",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: './chroma_db\\\\chroma.sqlite3'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\rag_chroma\\Lib\\shutil.py:632\u001b[39m, in \u001b[36m_rmtree_unsafe\u001b[39m\u001b[34m(path, onerror)\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m     os.unlink(fullname)\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[31mPermissionError\u001b[39m: [WinError 32] The process cannot access the file because it is being used by another process: './chroma_db\\\\chroma.sqlite3'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m     os.makedirs(path, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m reset successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mreset_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDB_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mreset_dir\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset_dir\u001b[39m(path):\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(path):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m         \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_readonly\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     os.makedirs(path, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m reset successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\rag_chroma\\Lib\\shutil.py:787\u001b[39m, in \u001b[36mrmtree\u001b[39m\u001b[34m(path, ignore_errors, onerror, dir_fd)\u001b[39m\n\u001b[32m    785\u001b[39m     \u001b[38;5;66;03m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[32m    786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\rag_chroma\\Lib\\shutil.py:634\u001b[39m, in \u001b[36m_rmtree_unsafe\u001b[39m\u001b[34m(path, onerror)\u001b[39m\n\u001b[32m    632\u001b[39m             os.unlink(fullname)\n\u001b[32m    633\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m             \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43munlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    636\u001b[39m     os.rmdir(path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mremove_readonly\u001b[39m\u001b[34m(func, path, excinfo)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03mClear the read-only attribute and retry deleting the file/folder.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03mThis is passed to shutil.rmtree's onerror parameter.\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m os.chmod(path, stat.S_IWRITE)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mPermissionError\u001b[39m: [WinError 32] The process cannot access the file because it is being used by another process: './chroma_db\\\\chroma.sqlite3'"
     ]
    }
   ],
   "source": [
    "# point to a fresh local folder:\n",
    "# (reset persistent data)\n",
    "DB_DIR = \"./chroma_db\"\n",
    "\n",
    "# function to handle read-only files during deletion\n",
    "def remove_readonly(func, path, excinfo):\n",
    "    \"\"\"\n",
    "    Clear the read-only attribute and retry deleting the file/folder.\n",
    "    This is passed to shutil.rmtree's onerror parameter.\n",
    "    \"\"\"\n",
    "    os.chmod(path, stat.S_IWRITE)\n",
    "    func(path)\n",
    "    \n",
    "def reset_dir(path):\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path, onerror=remove_readonly)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    print(f\"{path} reset successfully\")\n",
    "\n",
    "reset_dir(DB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7052a7-f3fc-4ba1-b575-ab17c6a7ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and move to GPU if available\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc7ff7-f4ac-4cb5-b2ca-e9cdcd66b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dimension\n",
    "def embedding_dim_for_model(m):\n",
    "    v = m.encode([\"hello\"], convert_to_numpy=True)\n",
    "    return v.shape[-1]\n",
    "\n",
    "print(\"Embedding dim: \", embedding_dim_for_model(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5109408-41d9-41b6-9281-bb77ab9637a8",
   "metadata": {},
   "source": [
    "## Initialize Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce7932-6563-4f4c-8a90-2c3733a208c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Chroma v2 PersistentClient\n",
    "client = chromadb.PersistentClient(path=DB_DIR)\n",
    "# create/get collection (new API)\n",
    "collection = client.get_or_create_collection(\"embed_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea7dec-a541-438d-89f1-ccf039fb5f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(question):\n",
    "    q_emb = model.encode([question], convert_to_numpy=True, device=DEVICE)\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=q_emb,\n",
    "        n_results=3\n",
    "    )\n",
    "    context = \"\\n\".join(results[\"documents\"][0])\n",
    "    answer = f\"Context:\\n{context}\\n\\nAnswer: (model answer here)\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256e3388-6b87-4794-b158-002da7edd441",
   "metadata": {},
   "source": [
    "## Load and Chunk Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50537c-1e53-4cb7-a10a-a0f54865793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import doc to augment prompt\n",
    "# md_path = \"rag_data.md\" \n",
    "md_path = \"fictional_company.md\"\n",
    "# clean up markdown file\n",
    "def md_to_plaintext(md):\n",
    "    # convert markdown to HTML\n",
    "    html = markdown2.markdown(md)\n",
    "    # strip HTML tags -> plain text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "    \n",
    "with open(md_path, \"r\", encoding=\"utf-8\") as f: # f is the file object\n",
    "    markdown_text = f.read() # read the file at md_path and store in this variable\n",
    "\n",
    "\n",
    "print(markdown_text[:500])\n",
    "print(\"Total characters:\", len(markdown_text))\n",
    "print(\"Total lines:\", len(markdown_text.splitlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c30f87-b471-4b9e-8669-1442017cb21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_text = md_to_plaintext(markdown_text) # convert markdown to plain text\n",
    "print(plain_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e8bd0-a913-409e-874f-67650ed44236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose tokenizer: mpnet can use \"cl100k_base\"\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# nltk.download('punkt_tab')\n",
    "sentences = nltk.sent_tokenize(plain_text)\n",
    "chunks = []\n",
    "current = \"\"\n",
    "for sent in sentences:\n",
    "    if len(enc.encode(current + sent)) < 120: # smaller chunks to increase granularity\n",
    "        current += \" \" + sent\n",
    "    else:\n",
    "        chunks.append(current.strip())\n",
    "        current = sent\n",
    "\n",
    "if current:\n",
    "    chunks.append(current.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f59cf70-e6ec-4287-8dbb-c2aa9af6726c",
   "metadata": {},
   "source": [
    "## Embed and Store Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe90c2-61ad-48b5-93dc-0bcca9576f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the chunks\n",
    "embeds = model.encode(chunks, convert_to_numpy=True, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d8e5d3-a9ee-4593-8477-89b550371ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add chunks and embeddings to Chroma\n",
    "collection.add(\n",
    "    documents=chunks,\n",
    "    embeddings=embeds.tolist(),\n",
    "    ids=[f\"chunk_{i}\" for i in range(len(chunks))],\n",
    ")\n",
    "\n",
    "print(\"Stored chunks in Chroma: \", collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349a98d3-620c-457a-afa0-7ad944971b6a",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d746fe-fdb6-4722-a1f7-42ed5021c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get relevant chunks\n",
    "def retrieve(query, k=2):\n",
    "    # embed the query\n",
    "    q_emb = model.encode([query], convert_to_numpy=True, device=DEVICE)\n",
    "    # query Chroma using embeddings\n",
    "    res = collection.query(query_embeddings=q_emb.tolist(), n_results=k)\n",
    "    # extract the retrieved documents\n",
    "    retrieved_chunks = res[\"documents\"][0] if \"documents\" in res else []\n",
    "\n",
    "    query_terms = set(query.lower().split())\n",
    "    \n",
    "    # minimal re-ranking:\n",
    "    retrieved_chunks = sorted(\n",
    "        retrieved_chunks,\n",
    "        key=lambda c: len(query_terms & set(c.lower().split())),\n",
    "        # key=lambda c: query.lower() in c.lower(),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    return retrieved_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f27f2-019f-4074-9423-2fef50150613",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve(\"What do we do with strong ideas?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1955c5e-eebc-4b90-9387-df9536f7b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the prompt with retrieved chunks and query\n",
    "def build_prompt(query, retrieved_chunks):\n",
    "    context = \"\\n\".join(retrieved_chunks)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are an expert agent at a company. \n",
    "Provide your answer about the company based on the given context.\n",
    "Combine the information from the retrieved documents to provide a concise, reasoned answer.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "RELEVANT SENTENCES:\n",
    "ANSWER:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909efdac-e966-479e-b387-fd7ecffb5fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_prompt(\"What do we do with strong ideas?\", retrieve(\"What do we do with strong ideas?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249775ca-b11f-486f-9217-ac671653c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the llm\n",
    "llm_model_name = \"google/flan-t5-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "llm_model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name).to(DEVICE)\n",
    "\n",
    "def generate(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "    outputs = llm_model.generate(**inputs, max_length=128) # more factual:  do_sample=False\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb67d1-4ac8-4793-84d8-071a7dd88680",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = min(8, collection.count())\n",
    "\n",
    "def rag(query, k):\n",
    "    retrieved = retrieve(query, k) # use Chroma retrieval\n",
    "        # print retrieved chunks for future analysis\n",
    "    print(\"\\n--- RETRIEVED CHUNKS ---\")\n",
    "    for i, chunk in enumerate(retrieved, 1):\n",
    "        print(f\"[{i}] {chunk}\\n\")\n",
    "            \n",
    "    prompt = build_prompt(query, retrieved) # combine query + context\n",
    "    answer = generate(prompt)     # call LLM\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb229c-761a-40d0-a324-eae5ddfd8230",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag(\"In 20 words or less, what does the company do?\", k)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e813c78-18cd-42e0-aee6-f2c91b6ca9aa",
   "metadata": {},
   "source": [
    "## Test 3 Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91493fa-5b6b-4282-bd31-4dc5231178c1",
   "metadata": {},
   "source": [
    "### Test Case 1 (Factual): A question directly answerable by the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e522477-29df-4b62-8306-9a398a3e96f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"What service does Apex Horizon Agency offer?\"\n",
    "result1 = rag(query1, k=5)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f80ac-73f4-46bd-a173-a9126ec2b142",
   "metadata": {},
   "source": [
    "### Test Case 2 (Foil/General): A question where the correct answer is not in the KB, but the LLM should rely on its general knowledge or state that it cannot answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb37d5-82c1-4c82-950e-5b9c9766199f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query2 = \"Can you mail things using USPS?\"\n",
    "result2 = rag(query2, k)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee07f22-a8f6-42a3-a27c-276c0ee62800",
   "metadata": {},
   "source": [
    "### Test Case 3 (Synthesis): A question requiring the LLM to combine information from multiple retrieved chunks or synthesize an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2013e1-c3af-49fc-b077-9972e8c3f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthesis question\n",
    "query3 = \"If I were looking for a modeling agent and I was also interested in publishing a book, what services could I expect Horizon Agency to provide?\"\n",
    "result3 = rag(query3, k=5)\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28167d88-2946-438b-8a36-14be7c4b9f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query4 = \"Do we take our clients seriously?\"\n",
    "result4 = rag(query4, k=5)\n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b29f8-1d43-44f6-af01-5a6ee1824ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "query5 = \"What do we do with strong ideas?\"\n",
    "result5 = rag(query5, k=5)\n",
    "print(result5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd3dfbf-3215-49dc-ad31-bd44cccd9f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_chroma)",
   "language": "python",
   "name": "rag_chroma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
